# Nima ğŸ¤–

> **A complete Large Language Model implementation from scratch**

Nima is a production-ready LLM framework built from first principles using PyTorch. It demonstrates transformer architecture, modern training techniques, and efficient inference with a clean, modular design.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- ğŸ—ï¸ **Complete Transformer Implementation**: Multi-head attention, feed-forward networks, and positional encoding from scratch
- ğŸ¨ **Multiple Architectures**: GPT-style decoder-only and full encoder-decoder models
- ğŸ“Š **Flexible Tokenization**: Character-level, word-level, and BPE tokenizers
- ğŸ¯ **Specialized Training**: Technical documentation, engineering content, and Q&A datasets
- ğŸš€ **Production Training**: Early stopping, learning rate scheduling, gradient accumulation
- ğŸ“ˆ **Advanced Monitoring**: TensorBoard, W&B integration, automatic visualization
- ğŸ’¬ **Advanced Generation**: Top-k, top-p, beam search, and temperature sampling
- ğŸ§ª **Comprehensive Evaluation**: Perplexity, accuracy, BLEU score, and sample generation
- ğŸ”§ **Easy to Extend**: Modular design makes experimentation simple

## ğŸ¯ Why Nima?

- **Educational**: Learn LLMs by building one from scratch
- **Practical**: Train real models on your own data
- **Customizable**: Modify any component to experiment with new ideas
- **Well-Documented**: Extensive documentation and examples

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ src/                    # Core implementation
â”‚   â”œâ”€â”€ models/            # Model architectures
â”‚   â”œâ”€â”€ data/              # Data processing
â”‚   â”œâ”€â”€ training/          # Training loops
â”‚   â”œâ”€â”€ evaluation/        # Evaluation metrics
â”‚   â”œâ”€â”€ inference/         # Inference engine
â”‚   â””â”€â”€ utils/             # Utility functions
â”œâ”€â”€ data/                  # Dataset storage
â”‚   â”œâ”€â”€ raw/              # Raw datasets
â”‚   â””â”€â”€ processed/        # Processed datasets
â”œâ”€â”€ configs/              # Configuration files
â”œâ”€â”€ experiments/          # Training experiments
â”‚   â”œâ”€â”€ checkpoints/      # Model checkpoints
â”‚   â””â”€â”€ logs/             # Training logs
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”œâ”€â”€ scripts/              # Utility scripts
â”œâ”€â”€ tests/                # Unit tests
â””â”€â”€ docs/                 # Documentation
```

## ğŸš€ Quick Start

### Basic Training (Tiny Shakespeare)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Prepare sample data
python scripts/prepare_data.py --dataset tiny_shakespeare --tokenizer char

# 3. Quick training test
python scripts/train.py --quick_test

# 4. Generate text
python scripts/inference.py \
  --checkpoint experiments/gpt_model/checkpoint_best.pt \
  --tokenizer data/processed/tiny_shakespeare/char_tokenizer.json \
  --prompt "Once upon a time"
```

### Technical Model Training (System Engineering)

```bash
# 1. View quick start guide
python scripts/example_technical_training.py

# 2. Prepare technical data
python scripts/prepare_technical_data.py \
  --output-dir data/processed/technical \
  --tokenizer bpe \
  --text-files data/raw/sample_k8s_doc.md \
  --json-files data/raw/technical_qa.json \
  --format qa

# 3. Train specialized model
python scripts/train_technical.py \
  --config configs/technical_training.yaml

# 4. Monitor training
tensorboard --logdir experiments/nima_technical/tensorboard

# 5. Evaluate and generate
python scripts/train_technical.py \
  --config configs/technical_training.yaml \
  --resume experiments/nima_technical/checkpoint_best.pt \
  --eval-only
```

## ğŸ§  Model Architecture

Our implementation includes:

- **Multi-Head Attention**: Core attention mechanism
- **Positional Encoding**: Position-aware embeddings
- **Feed-Forward Networks**: Transformer building blocks
- **Layer Normalization**: Training stability
- **Residual Connections**: Gradient flow optimization

## ğŸ“Š Implementation Status

### Core Architecture âœ…

- [x] Multi-head attention mechanism
- [x] Multiple transformer architectures (Encoder-Decoder, GPT-style)
- [x] Positional encoding (learned and sinusoidal)
- [x] Layer normalization and residual connections
- [x] Model factory with pre-configured sizes

### Data Processing âœ…

- [x] Three tokenizer types (char, word, BPE)
- [x] Efficient data loading and preprocessing
- [x] Technical data preparation pipeline
- [x] Multi-format support (text, markdown, JSON, JSONL)
- [x] 80/10/10 train/val/test splits

### Training âœ…

- [x] Training pipeline with checkpointing
- [x] Early stopping
- [x] Learning rate scheduling (warmup + cosine decay)
- [x] Gradient clipping and accumulation
- [x] Mixed precision training (FP16)
- [x] TensorBoard and W&B integration

### Evaluation âœ…

- [x] Comprehensive metrics (perplexity, accuracy, BLEU)
- [x] Automatic visualization (loss curves, plots)
- [x] Test set evaluation
- [x] Sample text generation for verification

### Inference âœ…

- [x] Advanced text generation (sampling strategies)
- [x] Top-k, top-p (nucleus), temperature sampling
- [x] Beam search
- [x] Interactive generation mode
- [x] Batch generation

### Coming Soon ğŸš§

- [ ] Distributed training (multi-GPU)
- [ ] Model quantization
- [ ] ONNX export
- [ ] Efficient attention (Flash Attention)
- [ ] Fine-tuning utilities

## ğŸ”§ Configuration

Model and training parameters are managed through YAML configuration files in the `configs/` directory:

- `base_model.yaml`: Basic model configuration
- `small_model.yaml`: Smaller model for quick experimentation
- `large_model.yaml`: Larger model for better performance

## ğŸ“ˆ Monitoring

Training progress can be monitored using:

- **TensorBoard**: Real-time training metrics
- **Weights & Biases**: Experiment tracking (optional)
- **Custom logging**: Detailed training logs

## ğŸ§ª Testing

Run the test suite:

```bash
pytest tests/
```

## ğŸ“ Training Specialized Models

Nima supports training on specialized domains like system engineering and technical documentation:

### Supported Data Types

- **Technical Documentation**: Kubernetes, Terraform, AWS, DevOps guides
- **Q&A Datasets**: StackOverflow-style technical questions and answers
- **Code Examples**: With syntax preservation for various languages
- **Engineering Blogs**: Technical articles and tutorials
- **Custom Notes**: Your own curated technical content

### Key Features

- **Smart Preprocessing**: Preserves code blocks, technical formatting, and commands
- **Multi-Source**: Combine multiple data sources with custom weights
- **Automatic Splits**: 80/10/10 train/validation/test splits with shuffling
- **Early Stopping**: Prevents overfitting with configurable patience
- **Comprehensive Monitoring**: TensorBoard, W&B, and automatic plot generation
- **Sample Generation**: Verify model quality with domain-specific prompts

### Example: Training on Kubernetes Documentation

```bash
# Prepare data
python scripts/prepare_technical_data.py \
  --output-dir data/processed/k8s \
  --tokenizer bpe \
  --text-files docs/k8s/*.md \
  --json-files data/raw/k8s_qa.json \
  --format qa

# Train
python scripts/train_technical.py \
  --config configs/technical_training.yaml

# Generate samples
python scripts/inference.py \
  --checkpoint experiments/nima_technical/checkpoint_best.pt \
  --tokenizer data/processed/k8s/tokenizer_bpe.json \
  --prompt "To deploy with Kubernetes"
```

See **[docs/training_technical.md](docs/training_technical.md)** for complete guide.

## ğŸ“š Learning Resources

### Documentation

- **[Getting Started](docs/getting_started.md)**: Quick start guide with examples
- **[Architecture](docs/architecture.md)**: Deep dive into transformer implementation
- **[Training Guide](docs/training.md)**: General training pipeline
- **[Technical Training](docs/training_technical.md)**: Specialized model training

### Notebooks

Check out the `notebooks/` directory for:

- Architecture deep dives
- Training tutorials
- Inference examples
- Performance analysis

## ğŸ¤ Contributing

This is a learning project! Feel free to:

1. Fork the repository
2. Create feature branches
3. Submit pull requests
4. Share improvements

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Learning Journey

Document your learning process and insights as you build and evolve this LLM. Each component teaches fundamental ML concepts that apply broadly in the field.

Happy coding! ğŸš€
