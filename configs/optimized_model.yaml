# Model Configuration
model:
  vocab_size: 10000
  d_model: 512
  n_heads: 8
  n_layers: 6
  d_ff: 2048
  max_seq_len: 1024
  dropout: 0.1

# Training Configuration - Optimized
training:
  batch_size: 64 # Increased for stability
  learning_rate: 0.0003 # Lower for better convergence
  num_epochs: 50 # Reduced for faster iteration
  warmup_steps: 2000 # Reduced proportionally
  weight_decay: 0.01
  gradient_clip: 1.0
  save_every: 1000
  eval_every: 500

# Data Configuration
data:
  dataset: "tiny_shakespeare"
  train_split: 0.9
  val_split: 0.1
  tokenizer_type: "char_level" # or "bpe", "word_level"

# Optimization
optimizer:
  type: "adam"
  betas: [0.9, 0.98]
  eps: 1e-9

# Scheduler
scheduler:
  type: "cosine_with_warmup"
  warmup_steps: 4000

# Logging
logging:
  use_wandb: false
  project_name: "llm_from_scratch"
  log_dir: "experiments/logs"

# Paths
paths:
  data_dir: "data"
  checkpoint_dir: "experiments/checkpoints"
  log_dir: "experiments/logs"
