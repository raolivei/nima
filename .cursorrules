# Nima - Cursor Rules

## Project Overview
Educational project for learning AI by building language models from scratch using PyTorch. Beginner-friendly transformer implementation.

## Project Structure
```
nima/
├── src/                    # Core AI model code
│   ├── models/            # Neural network architecture
│   ├── data/              # Text processing
│   ├── training/          # Training logic
│   └── inference/         # Text generation
├── scripts/               # Training and inference scripts
├── configs/               # YAML training configurations
├── experiments/          # Trained models and logs
└── data/                  # Training datasets
```

## Code Conventions

### Python Style
- Follow PEP 8
- Use type hints for clarity
- Docstrings for functions/classes
- Clear variable names (educational focus)
- Comment complex math/ML concepts

### File Organization
- `src/models/` - Core transformer architecture
- `src/training/` - Training loops and utilities
- `src/inference/` - Text generation logic
- `scripts/` - User-facing scripts (easy to run)
- `configs/` - YAML configs for different training runs

### Git Workflow
- Use conventional commits: `feat:`, `fix:`, `docs:`, `chore:`
- Feature branches: `feature/<name>`
- Keep experiments in `experiments/` (gitignored)

## Common Tasks

### Training a Model
1. Prepare data in `data/raw/` or `data/processed/`
2. Create/update config in `configs/`
3. Run: `python scripts/train.py --config configs/base_model.yaml`
4. Monitor with TensorBoard: `tensorboard --logdir experiments/logs`

### Testing Inference
1. Use trained checkpoint: `python scripts/ask_nima.py --checkpoint <path> --prompt "text"`
2. Checkpoint location: `experiments/<experiment_name>/checkpoint_best.pt`

### Adding New Dataset
1. Add raw data to `data/raw/`
2. Create processing script in `src/data/`
3. Process: `python scripts/prepare_data.py`
4. Update config to point to processed data

### Modifying Architecture
1. Edit files in `src/models/`
2. Update config if needed
3. Test with small dataset first
4. Document changes in docstrings

## Development Setup

### Installation
```bash
pip install -r requirements.txt
```

### Training Example
```bash
# Base model on Shakespeare
python scripts/train.py --config configs/base_model.yaml

# Technical content
python scripts/train_technical.py --config configs/technical_training.yaml
```

### Monitoring
```bash
tensorboard --logdir experiments/logs
# Open http://localhost:6006
```

## Key Concepts

### Transformer Architecture
- Attention mechanism
- Positional encoding
- Multi-head attention
- Feed-forward networks
- Layer normalization

### Training Process
- Loss function (cross-entropy)
- Optimizer (Adam)
- Learning rate scheduling
- Gradient clipping
- Checkpointing

### Evaluation Metrics
- Loss (lower is better)
- Perplexity (lower is better)
- Generated text quality

## Important Notes
- **Educational focus** - Code should be readable and well-commented
- **Small scale** - Designed for learning, not production
- **Experiments** - Keep in `experiments/` directory (gitignored)
- **Configs** - Use YAML for easy experimentation
- **TensorBoard** - Use for monitoring training progress

## Documentation
- `README.md` - Quick start and overview
- `docs/training.md` - Training guide
- `docs/architecture.md` - Model architecture
- `docs/getting_started.md` - Beginner guide

## Learning Path
1. **Beginner**: Run Shakespeare example, watch TensorBoard
2. **Intermediate**: Train on own data, experiment with configs
3. **Advanced**: Modify architecture, implement new features

